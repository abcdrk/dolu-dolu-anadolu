{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK Punkt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Using cached https://files.pythonhosted.org/packages/73/56/90178929712ce427ebad179f8dc46c8deef4e89d4c853092bee1efd57d05/nltk-3.4.1.zip\n",
      "Requirement already satisfied: six in /usr/local/Cellar/ipython/7.5.0/libexec/vendor/lib/python3.7/site-packages (from nltk) (1.12.0)\n",
      "Building wheels for collected packages: nltk\n",
      "  Building wheel for nltk (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/ozgedincsoy/Library/Caches/pip/wheels/97/8a/10/d646015f33c525688e91986c4544c68019b19a473cb33d3b55\n",
      "Successfully built nltk\n",
      "Installing collected packages: nltk\n",
      "Successfully installed nltk-3.4.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting pickle\n",
      "\u001b[31m  Could not find a version that satisfies the requirement pickle (from versions: )\u001b[0m\n",
      "\u001b[31mNo matching distribution found for pickle\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pickle\n",
    "import json\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "sent_detec = nltk.data.load('tokenizers/punkt/turkish.pickle')\n",
    "\n",
    "f = open(\"derlem.txt\", encoding=\"utf-16\", errors='ignore')\n",
    "corpus = f.read()\n",
    "par_list = corpus.split(\"\\n\\n\")\n",
    "\n",
    "par_dict = {}\n",
    "\n",
    "for par in par_list:\n",
    "    start = par.index(\" \")\n",
    "    num = par[:start]\n",
    "    key = int(num)\n",
    "    paragraph = par[start+1:]\n",
    "    sentences = sent_detec.tokenize(par[start+1:])\n",
    "    lemmas = []\n",
    "    for sentence in sentences:\n",
    "        result = analyze(sentence)\n",
    "        l = {}\n",
    "        for a in result.results:\n",
    "            if a.best.pos != 'Punc':\n",
    "                element = a.best.lemmas[0]\n",
    "                if a.best.lemmas[0] == 'UNK':\n",
    "                    element = a.token\n",
    "                count = l.get(element,0)\n",
    "                count += 1\n",
    "                l[element] = count\n",
    "        lemmas.append(l)\n",
    "                \n",
    "    par_dict[key] = { \"paragraph\" : paragraph, \"sentences\" : sentences, \"lemmas\" : lemmas}\n",
    "\n",
    "with open('parToSentences.pickle', 'wb') as handle:\n",
    "    pickle.dump(par_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'paragraph': 'Madenler, yaşamımızın birçok alanında çeşitli amaçlarla işleyerek kullandığımız doğal kaynaklardır. İnşaat, otomotiv, beyaz eşya, mutfak eşyaları, elektrikli ve elektronik eşya üretimi gibi birçok alanda madenler kullanılmaktadır.', 'sentences': ['Madenler, yaşamımızın birçok alanında çeşitli amaçlarla işleyerek kullandığımız doğal kaynaklardır.', 'İnşaat, otomotiv, beyaz eşya, mutfak eşyaları, elektrikli ve elektronik eşya üretimi gibi birçok alanda madenler kullanılmaktadır.'], 'lemmas': [{'maden': 1, 'yaşam': 1, 'birçok': 1, 'alan': 1, 'çeşit': 1, 'amaç': 1, 'işle': 1, 'kullan': 1, 'doğal': 1, 'kaynakla': 1}, {'inşaat': 1, 'otomotiv': 1, 'beyaz': 1, 'eşya': 3, 'mutfak': 1, 'elektrik': 1, 've': 1, 'elektronik': 1, 'üretim': 1, 'gibi': 1, 'birçok': 1, 'alan': 1, 'maden': 1, 'kullan': 1}]}\n",
      "{'paragraph': 'Türkiye, maden çeşidi bakımından zengin bir ülkedir. Bu maden yataklarının önemli bir kısmı işlenmektedir. Bir maden yatağı işletmeye açılırken o madenin toplam miktarı yani rezervi belirlenir. Maden içindeki saf metal oranı olan tenör de bir maden yatağının işletilmeye açılıp açılmamasında önem taşıyan özelliklerdendir. Çünkü miktarı fazla olsa da tenörü çok düşük olan yataklar işletmeye açılmamaktadır. Bunların yanı sıra işletme maliyeti hesaplanmakta ve kârlı olabilecek yataklar işletilmektedir.', 'sentences': ['Türkiye, maden çeşidi bakımından zengin bir ülkedir.', 'Bu maden yataklarının önemli bir kısmı işlenmektedir.', 'Bir maden yatağı işletmeye açılırken o madenin toplam miktarı yani rezervi belirlenir.', 'Maden içindeki saf metal oranı olan tenör de bir maden yatağının işletilmeye açılıp açılmamasında önem taşıyan özelliklerdendir.', 'Çünkü miktarı fazla olsa da tenörü çok düşük olan yataklar işletmeye açılmamaktadır.', 'Bunların yanı sıra işletme maliyeti hesaplanmakta ve kârlı olabilecek yataklar işletilmektedir.'], 'lemmas': [{'türkiye': 1, 'maden': 1, 'çeşit': 1, 'bakım': 1, 'zengin': 1, 'bir': 1, 'ülke': 1}, {'bu': 1, 'maden': 1, 'yatak': 1, 'önem': 1, 'bir': 1, 'kısım': 1, 'işle': 1}, {'bir': 1, 'maden': 2, 'yatak': 1, 'işle': 1, 'aç': 1, 'o': 1, 'toplam': 1, 'miktar': 1, 'yani': 1, 'rezerv': 1, 'belirle': 1}, {'maden': 2, 'iç': 1, 'saf': 1, 'metal': 1, 'oran': 1, 'ol': 1, 'UNK': 1, 'de': 1, 'bir': 1, 'yatak': 1, 'işle': 1, 'aç': 2, 'önem': 1, 'taşı': 1, 'özellik': 1}, {'çünkü': 1, 'miktar': 1, 'fazla': 1, 'ol': 2, 'da': 1, 'UNK': 1, 'çok': 1, 'düşük': 1, 'yatak': 1, 'işle': 1, 'aç': 1}, {'bu': 1, 'yan': 1, 'sıra': 1, 'işle': 2, 'maliyet': 1, 'hesapla': 1, 've': 1, 'kar': 1, 'ol': 1, 'yatak': 1}]}\n",
      "{'paragraph': 'Ülkemizde çıkarılan madenlerden bazıları metaliktir: demir, bakır, krom, kurşun ve çinko gibi. Bor, kükürt, asbest, fosfat gibi bazı madenler ise metalik değildir. Şimdi ülkemizdeki başlıca madenleri inceleyelim.', 'sentences': ['Ülkemizde çıkarılan madenlerden bazıları metaliktir: demir, bakır, krom, kurşun ve çinko gibi.', 'Bor, kükürt, asbest, fosfat gibi bazı madenler ise metalik değildir.', 'Şimdi ülkemizdeki başlıca madenleri inceleyelim.'], 'lemmas': [{'ülke': 1, 'çıkar': 1, 'maden': 1, 'bazı': 1, 'metalik': 1, 'demir': 1, 'bakır': 1, 'krom': 1, 'kurşun': 1, 've': 1, 'çinko': 1, 'gibi': 1}, {'bor': 1, 'kükürt': 1, 'asbest': 1, 'fosfat': 1, 'gibi': 1, 'bazı': 1, 'maden': 1, 'i': 1, 'metalik': 1, 'değil': 1}, {'şimdi': 1, 'ülke': 1, 'başlıca': 1, 'maden': 1, 'incele': 1}]}\n",
      "{'paragraph': 'Demir çelik sanayisinin temel ham maddesi olan ve ülkemizde en çok tüketilen madenlerden biri demirdir. Demir rezervi bakımından en zengin yataklar, Sivas ve Malatya illerinde bulunmaktadır. Divriği, Hasançelebi ve Hekimhan, bu illerde demir yataklarının bulunduğu başlıca yerlerdir. Diğer önemli demir yatakları Niğde, Bingöl ve Kesikköprü’dedir (Ankara).Bu yerlerden çıkarılan demir Ereğli (Zonguldak), Karabük ve İskenderun demir çelik fabrikalarında işlenmektedir.', 'sentences': ['Demir çelik sanayisinin temel ham maddesi olan ve ülkemizde en çok tüketilen madenlerden biri demirdir.', 'Demir rezervi bakımından en zengin yataklar, Sivas ve Malatya illerinde bulunmaktadır.', 'Divriği, Hasançelebi ve Hekimhan, bu illerde demir yataklarının bulunduğu başlıca yerlerdir.', 'Diğer önemli demir yatakları Niğde, Bingöl ve Kesikköprü’dedir (Ankara).Bu yerlerden çıkarılan demir Ereğli (Zonguldak), Karabük ve İskenderun demir çelik fabrikalarında işlenmektedir.'], 'lemmas': [{'demir': 2, 'çelik': 1, 'sanayi': 1, 'temel': 1, 'ham': 1, 'madde': 1, 'ol': 1, 've': 1, 'ülke': 1, 'en': 1, 'çok': 1, 'tüket': 1, 'maden': 1, 'biri': 1}, {'demir': 1, 'rezerv': 1, 'bakım': 1, 'en': 1, 'zengin': 1, 'yatak': 1, 'sivas': 1, 've': 1, 'malatya': 1, 'il': 1, 'bulun': 1}, {'divriği': 1, 'hasançelebi': 1, 've': 1, 'hekimhan': 1, 'bu': 1, 'il': 1, 'demir': 1, 'yatak': 1, 'bulun': 1, 'başlıca': 1, 'ye': 1}, {'diğer': 1, 'önem': 1, 'demir': 3, 'yatak': 1, 'niğde': 1, 'bingöl': 1, 've': 2, 'kesikköprü': 1, 'ankara': 1, 'bu': 1, 'yer': 1, 'çıkar': 1, 'ereğli': 1, 'zonguldak': 1, 'karabük': 1, 'iskenderun': 1, 'çelik': 1, 'fabrika': 1, 'işle': 1}]}\n",
      "{'paragraph': 'Krom, demir çelik sanayisinin önemli bir ham maddesidir. Çeliğin sertleştirilmesi ve paslanmaz çelik üretiminin yanı sıra mutfak eşyaları, sokak lambaları, denizaltı, gemi, uçak ve boya sanayisinde de krom kullanılmaktadır. Türkiye, krom yatakları bakımından zengin bir ülkedir. Çok sayıda krom yatağı bulunan ülkemizdeki başlıca krom yatakları Fethiye, Köyceğiz (Muğla), Guleman (Elâzığ), Kop Dağı (Bayburt), Mersin ile Kayseri arasında, Balıkesir ile Eskişehir arasında, İskenderun ile Kahramanmaraş arasında bulunmaktadır. Dünyanın en çok krom üreten ülkeleri arasında bulunan Türkiye, krom üretiminin bir kısmını ihraç etmektedir. Ülkemizde krom işleyen fabrikalar, Elâzığ ve Antalya’da bulunmaktadır.', 'sentences': ['Krom, demir çelik sanayisinin önemli bir ham maddesidir.', 'Çeliğin sertleştirilmesi ve paslanmaz çelik üretiminin yanı sıra mutfak eşyaları, sokak lambaları, denizaltı, gemi, uçak ve boya sanayisinde de krom kullanılmaktadır.', 'Türkiye, krom yatakları bakımından zengin bir ülkedir.', 'Çok sayıda krom yatağı bulunan ülkemizdeki başlıca krom yatakları Fethiye, Köyceğiz (Muğla), Guleman (Elâzığ), Kop Dağı (Bayburt), Mersin ile Kayseri arasında, Balıkesir ile Eskişehir arasında, İskenderun ile Kahramanmaraş arasında bulunmaktadır.', 'Dünyanın en çok krom üreten ülkeleri arasında bulunan Türkiye, krom üretiminin bir kısmını ihraç etmektedir.', 'Ülkemizde krom işleyen fabrikalar, Elâzığ ve Antalya’da bulunmaktadır.'], 'lemmas': [{'krom': 1, 'demir': 1, 'çelik': 1, 'sanayi': 1, 'önem': 1, 'bir': 1, 'ham': 1, 'madde': 1}, {'çelik': 2, 'sert': 1, 've': 2, 'paslan': 1, 'üretim': 1, 'yan': 1, 'sıra': 1, 'mutfak': 1, 'eşya': 1, 'sokak': 1, 'lamba': 1, 'denizaltı': 1, 'gemi': 1, 'uçak': 1, 'boya': 1, 'sanayi': 1, 'de': 1, 'krom': 1, 'kullan': 1}, {'türkiye': 1, 'krom': 1, 'yatak': 1, 'bakım': 1, 'zengin': 1, 'bir': 1, 'ülke': 1}, {'çok': 1, 'sayı': 1, 'krom': 2, 'yatak': 2, 'bulun': 2, 'ülke': 1, 'başlıca': 1, 'fethiye': 1, 'köyceğiz': 1, 'muğla': 1, 'UNK': 1, 'elazığ': 1, 'kop': 1, 'dağ': 1, 'bayburt': 1, 'mersin': 1, 'ile': 3, 'kayseri': 1, 'ara': 3, 'balıkesir': 1, 'eskişehir': 1, 'iskenderun': 1, 'kahramanmaraş': 1}, {'dünya': 1, 'en': 1, 'çok': 1, 'krom': 2, 'üre': 1, 'ülke': 1, 'ara': 1, 'bulun': 1, 'türkiye': 1, 'üretim': 1, 'bir': 1, 'kısım': 1, 'ihraç': 1, 'et': 1}, {'ülke': 1, 'krom': 1, 'işle': 1, 'fabrika': 1, 'elazığ': 1, 've': 1, 'antalya': 1, 'bulun': 1}]}\n"
     ]
    }
   ],
   "source": [
    "aa = 0\n",
    "for e in par_dict:\n",
    "    print(par_dict[e])\n",
    "    aa += 1\n",
    "    if aa == 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Docker Installation for Zemberek"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "brew cask install docker\n",
    "\n",
    "open --background -a Docker\n",
    "\n",
    "sudo docker run -d --rm -p 6789:6789 --name zemberek-grpc ryts/zemberek-grpc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zemberek "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: zemberek-grpc in /usr/local/lib/python3.7/site-packages (0.16.1)\n",
      "Requirement already satisfied: grpcio in /usr/local/lib/python3.7/site-packages (from zemberek-grpc) (1.20.1)\n",
      "Requirement already satisfied: grpcio-tools in /usr/local/lib/python3.7/site-packages (from zemberek-grpc) (1.20.1)\n",
      "Requirement already satisfied: googleapis-common-protos in /usr/local/lib/python3.7/site-packages (from zemberek-grpc) (1.6.0)\n",
      "Requirement already satisfied: six>=1.5.2 in /usr/local/Cellar/ipython/7.5.0/libexec/vendor/lib/python3.7/site-packages (from grpcio->zemberek-grpc) (1.12.0)\n",
      "Requirement already satisfied: protobuf>=3.5.0.post1 in /usr/local/lib/python3.7/site-packages (from grpcio-tools->zemberek-grpc) (3.7.1)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/site-packages (from protobuf>=3.5.0.post1->grpcio-tools->zemberek-grpc) (40.8.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install zemberek-grpc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import sys\n",
    "\n",
    "import grpc\n",
    "\n",
    "import zemberek_grpc.language_id_pb2 as z_langid\n",
    "import zemberek_grpc.language_id_pb2_grpc as z_langid_g\n",
    "import zemberek_grpc.normalization_pb2 as z_normalization\n",
    "import zemberek_grpc.normalization_pb2_grpc as z_normalization_g\n",
    "import zemberek_grpc.preprocess_pb2 as z_preprocess\n",
    "import zemberek_grpc.preprocess_pb2_grpc as z_preprocess_g\n",
    "import zemberek_grpc.morphology_pb2 as z_morphology\n",
    "import zemberek_grpc.morphology_pb2_grpc as z_morphology_g\n",
    "\n",
    "channel = grpc.insecure_channel('localhost:6789')\n",
    "\n",
    "langid_stub = z_langid_g.LanguageIdServiceStub(channel)\n",
    "normalization_stub = z_normalization_g.NormalizationServiceStub(channel)\n",
    "preprocess_stub = z_preprocess_g.PreprocessingServiceStub(channel)\n",
    "morphology_stub = z_morphology_g.MorphologyServiceStub(channel)\n",
    "\n",
    "def find_lang_id(i):\n",
    "    response = langid_stub.Detect(z_langid.LanguageIdRequest(input=i))\n",
    "    return response.langId\n",
    "\n",
    "def tokenize(i):\n",
    "    response = preprocess_stub.Tokenize(z_preprocess.TokenizationRequest(input=i))\n",
    "    return response.tokens\n",
    "\n",
    "def normalize(i):\n",
    "    response = normalization_stub.Normalize(z_normalization.NormalizationRequest(input=i))\n",
    "    return response\n",
    "\n",
    "def analyze(i):\n",
    "    response = morphology_stub.AnalyzeSentence(z_morphology.SentenceAnalysisRequest(input=i))\n",
    "    return response;\n",
    "\n",
    "def fix_decode(text):\n",
    "    \"\"\"Pass decode.\"\"\"\n",
    "    if sys.version_info < (3, 0):\n",
    "        return text.decode('utf-8')\n",
    "    else:\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import random\n",
    "import os\n",
    "import collections\n",
    "from pathlib import Path\n",
    "\n",
    "cwd = os.getcwd()\n",
    "\n",
    "GROUP_FILE_NAME = '2.txt'\n",
    "\n",
    "# _, test_questions_path, task1_pred_path, task2_pred_path = sys.argv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we do not take the argument from command line yet\n",
    "task1_pred_path = cwd + \"/task1_predictions\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatizer from @akoksal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "class Lemmatizer:\n",
    "    def __init__(self):\n",
    "        try:\n",
    "            with open('revisedDict.pkl', 'rb') as f:\n",
    "                self.revisedDict = pickle.load(f)\n",
    "        except IOError:\n",
    "            print(\"Please run trainLexicon.py to generate revisedDict.pkl file\")\n",
    "\n",
    "    def check(self, root, suffix, guess, action):\n",
    "        if action == \"unsuz yumusamasi\":\n",
    "            return len(suffix)>0 and suffix[0] in [\"a\",\"e\",\"ı\",\"i\",\"o\",\"ö\",\"u\",\"ü\"] and self.checkSuffixValidation(suffix)[0]\n",
    "        if action == \"unlu daralmasi\":\n",
    "            if guess==\"demek\" and self.checkSuffixValidation(suffix)[0]:\n",
    "                return True\n",
    "            if guess==\"yemek\" and self.checkSuffixValidation(suffix)[0]:\n",
    "                return True\n",
    "            \n",
    "            if suffix.startswith(\"yor\"):\n",
    "                lastVowel = \"\"\n",
    "                for letter in reversed(guess[:-3]):\n",
    "                    if letter in [\"a\",\"e\",\"ı\",\"i\",\"o\",\"ö\",\"u\",\"ü\"]:\n",
    "                        lastVowel = letter\n",
    "                        break\n",
    "                if lastVowel in [\"a\",\"e\"] and self.checkSuffixValidation(suffix)[0]:\n",
    "                    return True\n",
    "            return False\n",
    "        if action == \"fiil\" or action == \"olumsuzluk eki\":\n",
    "            return self.checkSuffixValidation(suffix)[0] and not ((root.endswith(\"la\") or (root.endswith(\"le\"))) and suffix.startswith(\"r\"))\n",
    "        if action == \"unlu dusmesi\":\n",
    "            count = 0\n",
    "            for letter in guess:\n",
    "                if letter in [\"a\",\"e\",\"ı\",\"i\",\"o\",\"ö\",\"u\",\"ü\"]:\n",
    "                    count+=1\n",
    "                    lastVowel = letter\n",
    "            if self.checkSuffixValidation(suffix)[0] and count==2 and (lastVowel in [\"ı\",\"i\",\"u\",\"ü\"]) and (len(suffix)>0 and suffix[0] in [\"a\",\"e\",\"ı\",\"i\",\"o\",\"ö\",\"u\",\"ü\"]):\n",
    "                if lastVowel == \"ı\":\n",
    "                    return suffix[0] in [\"a\",\"ı\"]\n",
    "                elif lastVowel == \"i\":\n",
    "                    return suffix[0] in [\"e\",\"i\"]\n",
    "                elif lastVowel == \"u\":\n",
    "                    return suffix[0] in [\"a\",\"u\"]\n",
    "                elif lastVowel == \"ü\":\n",
    "                    return suffix[0] in [\"e\",\"ü\"]\n",
    "            return False\n",
    "        return True\n",
    "        \n",
    "    def findPos(self, kelime):\n",
    "        revisedDict = self.revisedDict\n",
    "        l = []\n",
    "        if \"'\" in kelime:\n",
    "            l.append([kelime[:kelime.index(\"'\")]+\"_1\",\"tirnaksiz\",kelime])\n",
    "        mid = []\n",
    "        for i in range(len(kelime)):\n",
    "            guess = kelime[:len(kelime)-i]\n",
    "            suffix = kelime[len(kelime)-i:]\n",
    "            ct = 1\n",
    "            \n",
    "            while guess+\"_\"+str(ct) in revisedDict:\n",
    "                if self.check(guess, suffix, revisedDict[guess+\"_\"+str(ct)][1], revisedDict[guess+\"_\"+str(ct)][0]):\n",
    "                    guessList = (revisedDict[guess+\"_\"+str(ct)])\n",
    "                    while guessList[0] not in [\"kok\",\"fiil\",\"olumsuzluk\"]:\n",
    "                        guessList = revisedDict[guessList[1]]\n",
    "                    mid.append([guessList[1], revisedDict[guess+\"_\"+str(ct)][0],guess+\"_\"+str(ct)])\n",
    "                ct = ct+1\n",
    "                \n",
    "        temp = []\n",
    "        for kel in mid:\n",
    "            kelime_kok = kel[0][:kel[0].index(\"_\")]\n",
    "            kelime_len = len(kelime_kok)\n",
    "            if kelime_kok.endswith(\"mak\") or kelime_kok.endswith(\"mek\"):\n",
    "                kelime_len -= 3\n",
    "            not_inserted = True\n",
    "            for index in range(len(temp)):\n",
    "                temp_kelime = temp[index]\n",
    "                temp_kelime_kok = temp_kelime[0][:temp_kelime[0].index(\"_\")]\n",
    "                temp_len = len(temp_kelime_kok)\n",
    "                if temp_kelime_kok.endswith(\"mak\") or temp_kelime_kok.endswith(\"mek\"):\n",
    "                    temp_len -= 3\n",
    "                if(kelime_len>temp_len):\n",
    "                    temp.insert(index,kel)\n",
    "                    not_inserted = False\n",
    "            if not_inserted:\n",
    "                temp.append(kel)\n",
    "        output = l+temp\n",
    "        if len(output)==0:\n",
    "            output.append([kelime+\"_1\",\"çaresiz\",kelime+\"_1\",])\n",
    "        return output\n",
    "\n",
    "    def checkSuffixValidation(self, suff):\n",
    "        suffixList = [\"\",\"a\", \"abil\", \"acağ\", \"acak\", \"alım\", \"ama\", \"an\", \"ar\", \"arak\", \"asın\", \"asınız\", \"ayım\", \"da\", \"dan\", \"de\", \"den\", \"dı\", \"dığ\", \"dık\", \"dıkça\", \"dır\", \"di\", \"diğ\", \"dik\", \"dikçe\", \"dir\", \"du\", \"duğ\", \"duk\", \"dukça\", \"dur\", \"dü\", \"düğ\", \"dük\", \"dükçe\", \"dür\", \"e\", \"ebil\", \"eceğ\", \"ecek\", \"elim\", \"eme\", \"en\", \"er\", \"erek\", \"esin\", \"esiniz\", \"eyim\", \"ı\", \"ıl\", \"ım\", \"ımız\", \"ın\", \"ınca\", \"ınız\", \"ıp\", \"ır\", \"ıyor\", \"ız\", \"i\", \"il\", \"im\", \"imiz\", \"in\", \"ince\", \"iniz\", \"ip\", \"ir\", \"iyor\", \"iz\", \"k\", \"ken\", \"la\", \"lar\", \"ları\", \"ların\", \"le\", \"ler\", \"leri\", \"lerin\", \"m\", \"ma\", \"madan\", \"mak\", \"maksızın\", \"makta\", \"maktansa\", \"malı\", \"maz\", \"me\", \"meden\", \"mek\", \"meksizin\", \"mekte\", \"mektense\", \"meli\", \"mez\", \"mı\", \"mış\", \"mız\", \"mi\", \"miş\", \"miz\", \"mu\", \"muş\", \"mü\", \"muz\", \"müş\", \"müz\", \"n\", \"nın\", \"nız\", \"nin\", \"niz\", \"nun\", \"nuz\", \"nün\", \"nüz\", \"r\", \"sa\", \"se\", \"sı\", \"sın\", \"sınız\", \"sınlar\", \"si\", \"sin\", \"siniz\", \"sinler\", \"su\", \"sun\", \"sunlar\", \"sunuz\", \"sü\", \"sün\", \"sünler\", \"sünüz\", \"ta\", \"tan\", \"te\", \"ten\", \"tı\", \"tığ\", \"tık\", \"tıkça\", \"tır\", \"ti\", \"tiğ\", \"tik\", \"tikçe\", \"tir\", \"tu\", \"tuğ\", \"tuk\", \"tukça\", \"tur\", \"tü\", \"tüğ\", \"tük\", \"tükçe\", \"tür\", \"u\", \"ul\", \"um\", \"umuz\", \"un\", \"unca\", \"unuz\", \"up\", \"ur\", \"uyor\", \"uz\", \"ü\", \"ül\", \"ün\", \"üm\", \"ümüz\", \"ünce\", \"ünüz\", \"üp\", \"ür\", \"üyor\", \"üz\", \"ya\", \"yabil\", \"yacağ\", \"yacak\", \"yalım\", \"yama\", \"yan\", \"yarak\", \"yasın\", \"yasınız\", \"yayım\", \"ydı\", \"ydi\", \"ydu\", \"ydü\", \"ye\", \"yebil\", \"yeceğ\", \"yecek\", \"yelim\", \"yeme\", \"yen\", \"yerek\", \"yesin\", \"yesiniz\", \"yeyim\", \"yı\", \"yım\", \"yın\", \"yınca\", \"yınız\", \"yıp\", \"yız\", \"yi\", \"yim\", \"yin\", \"yince\", \"yiniz\", \"yip\", \"yiz\", \"yken\", \"yla\", \"yle\", \"ymış\", \"ymiş\", \"ymuş\", \"ymüş\", \"yor\", \"ysa\", \"yse\", \"yu\", \"yum\", \"yun\", \"yunca\", \"yunuz\", \"yup\", \"yü\", \"yuz\", \"yüm\", \"yün\", \"yünce\", \"yünüz\", \"yüp\", \"yüz\"]\n",
    "        validList = []\n",
    "        if suff in suffixList:\n",
    "            validList.append(suff)\n",
    "        for ind in range(1,len(suff)):\n",
    "            if(suff[:ind] in suffixList):\n",
    "                cont, contList = self.checkSuffixValidation(suff[ind:])\n",
    "                if cont:\n",
    "                    contList = [suff[:ind]+\"+\"+l for l in contList]\n",
    "                    validList = validList+contList\n",
    "        return len(validList)>0,validList\n",
    "\n",
    "    def lemmatize(self, text):\n",
    "        dc  = []\n",
    "        for i in text.split(' '):\n",
    "            findings = self.findPos(i.lower())\n",
    "            word = findings[0][0]\n",
    "            dc.append(findings[0][0][:-2])\n",
    "    \n",
    "        return dc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from string import punctuation\n",
    "\n",
    "def containArr(arr1, arr2):\n",
    "    \"\"\"returns true if at least one element from arr1 is contained by arr2\"\"\"\n",
    "    for element in arr1:\n",
    "        if element in arr2:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    \"\"\"\n",
    "    Removes punctuations from given text\n",
    "    \"\"\"\n",
    "    # Replace punctuation with space instead of remove it for hand-to-mouth, six-week-old, euro-certificate\n",
    "    return text.translate(str.maketrans(punctuation, ' ' * len(punctuation)))\n",
    "\n",
    "\n",
    "def remove_extra_whitespaces(text):\n",
    "    \"\"\"\n",
    "    Removes extra whitespaces from given text such as multiple adjencent space\n",
    "    \"\"\"\n",
    "    return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "\n",
    "def replace_capital_i(text):\n",
    "    \"\"\"\n",
    "    Replaces İ with i since it gave error on @akoksal 's script.\n",
    "    \"\"\"\n",
    "    return text.replace(\"İ\", \"i\")\n",
    "\n",
    "\n",
    "def normalize(text):\n",
    "    \"\"\"\n",
    "    Normalizes the text.\n",
    "    \"\"\"\n",
    "    return remove_extra_whitespaces(remove_punctuation(replace_capital_i(text))).lower()\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    Returns a word array from the given text.\n",
    "    \"\"\"\n",
    "    foo = text.split(' ')\n",
    "    return foo\n",
    "\n",
    "\n",
    "def tokenize_lemmatize(text):\n",
    "    \"\"\"\n",
    "    Returns a word array from the given text.\n",
    "    \"\"\"\n",
    "    foo = text.split(' ')\n",
    "    new_items = [lemmatizer.lemmatize(i)[0] for i in foo]\n",
    "    return new_items\n",
    "\n",
    "\n",
    "def find_sentence(que, par):\n",
    "    \"\"\"\n",
    "    Returns the related sentence to the question in the paragraph.\n",
    "    \"\"\"\n",
    "    sent = par\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Database\n",
    "## Read derlem.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re\n",
    "\n",
    "'''\n",
    "Transfer DERLEM to a Dictionary.\n",
    "'''\n",
    "with open(\"derlem.txt\", encoding=\"utf-16\") as f:\n",
    "    opened_file = f.readlines()\n",
    "\n",
    "'''\n",
    "The dictionary of the DERLEM.\n",
    "{id: paragraph as string}\n",
    "'''\n",
    "file_dict = {}\n",
    "for i, line in enumerate(opened_file):\n",
    "    if line != \"\\n\":\n",
    "        '''\n",
    "        The first number in the line is the Paragraph ID,\n",
    "        assign the Paragraph ID as the key to the dictionary,\n",
    "        and the rest of the paragraph to the value.\n",
    "        '''\n",
    "        file_dict[int(line.split(' ', 1)[0])] = re.findall(\" (.*)\", line)[0]\n",
    "    \n",
    "# print(len(file_dict))\n",
    "\n",
    "'''\n",
    "The normalized dictionary of the DERLEM.\n",
    "{id: array of tokens}\n",
    "'''\n",
    "norm_dict = file_dict.copy()\n",
    "lemmatizer = Lemmatizer()\n",
    "for i in norm_dict:\n",
    "    norm_dict[i] = lemmatizer.lemmatize(normalize(file_dict[i]))\n",
    "    \n",
    "# print(norm_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read soru_gruplari.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the encoding of the input file to latin-1\n",
    "file_content = \"\"\n",
    "with open(\"soru_gruplari.txt\", encoding=\"utf-16\") as f:\n",
    "    file_content = f.read()\n",
    "# Read the file and append all of the lines\n",
    "question_groups = file_content.split(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_groups_dict = {}\n",
    "for group in question_groups:\n",
    "    lines = group.split(\"\\n\")\n",
    "    answer_id = \"\"\n",
    "    group_dict = collections.defaultdict(dict)\n",
    "    for line in lines:\n",
    "        ID = re.findall(\"(.*):\", line)[0]\n",
    "        sentence = re.findall(\": (.*)\", line)[0]\n",
    "        if ID[0] == 'S':  # if the sentence is a question\n",
    "            group_dict[\"questions\"][ID] = sentence\n",
    "        elif ID[0] == 'C':  # if the sentence is the answer\n",
    "            group_dict[\"answer\"] = sentence\n",
    "            answer_id = ID\n",
    "        else:  # if the sentence is the id of related paragraph\n",
    "            group_dict[\"paragraph\"] = sentence\n",
    "    question_groups_dict[answer_id] = group_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraphs_and_terms = {}\n",
    "all_terms = set()\n",
    "for paragraph_id in norm_dict:\n",
    "    terms = norm_dict[paragraph_id]\n",
    "    all_terms.update(terms)\n",
    "    paragraphs_and_terms[paragraph_id] = terms\n",
    "all_terms = list(all_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute term frequency for each term and paragraph\n",
    "tf = {}\n",
    "for paragraph_id in norm_dict:\n",
    "    for term in norm_dict[paragraph_id]:\n",
    "        key = (paragraph_id, term)\n",
    "        tf[key] = norm_dict[paragraph_id].count(term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute document frequency for each term\n",
    "df = {}\n",
    "for term in all_terms:\n",
    "    df[term] = sum([1 for paragraph_id in norm_dict if norm_dict[paragraph_id].count(term) > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tf.idf score for a given query (q) and the set of paragraphs (documents - d) is $\\sum\\limits_{t\\in q\\cap d} tf.idf_{t,d}$ where\n",
    "\n",
    "tf score = $\\begin{cases}\n",
    "  1 + log_{10}tf_{t, d}, & \\text{if $tf_{t, d} > 0$}\\\\\n",
    "  0, & \\text{otherwise}\n",
    "\\end{cases}  $\n",
    "\n",
    "and\n",
    "\n",
    "idf score = $log_{10}(N / df_{t})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import collections\n",
    "\n",
    "# Compute tf-idf scores for each paragraph\n",
    "weights = collections.defaultdict(list)\n",
    "for paragraph_id in norm_dict:\n",
    "    for term in all_terms:\n",
    "        if((paragraph_id, term) in tf):\n",
    "            weights[paragraph_id].append((1 + math.log10(tf[(paragraph_id, term)])) * math.log10(len(norm_dict)/df[term]))\n",
    "        else:\n",
    "            weights[paragraph_id].append(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_related_paragraph(question):\n",
    "    lemmatizer = Lemmatizer()\n",
    "    query_terms = lemmatizer.lemmatize(normalize(question))\n",
    "    \n",
    "    query_term_indices = [all_terms.index(term) for term in query_terms if term in all_terms]\n",
    "\n",
    "    tf_idf = {}\n",
    "    for paragraph_id in weights:\n",
    "        tf_idf[paragraph_id] = sum([weight for index, weight in enumerate(weights[paragraph_id]) if index in query_term_indices])\n",
    "\n",
    "    related_paragraphs = sorted(tf_idf.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return related_paragraphs[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_answer(question):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write predicted results to files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/dilrubareyyan/Codes/dolu-dolu-anadolu/test_questions.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-d0e5f69231d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mrelated_paragraphs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0manswers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcwd\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/test_questions.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf-16\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mquestions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mquestion\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mquestions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/dilrubareyyan/Codes/dolu-dolu-anadolu/test_questions.txt'"
     ]
    }
   ],
   "source": [
    "related_paragraphs = []\n",
    "answers = []\n",
    "with open(cwd + '/test_questions.txt', 'r', encoding=\"utf-16\") as f:\n",
    "    questions = f.readlines()\n",
    "    for question in questions:\n",
    "        paragraph_id = predict_related_paragraph(question)\n",
    "        answer = predict_answer(question)\n",
    "        related_paragraphs.append(str(paragraph_id))\n",
    "        answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the following line if you are running this for the first time\n",
    "# os.mkdir(task1_pred_path)\n",
    "\n",
    "with open(Path(task1_pred_path) / GROUP_FILE_NAME, 'w+', encoding='utf16') as f:\n",
    "    f.write('\\n'.join(related_paragraphs))\n",
    "\n",
    "# TODO: complete task 2    \n",
    "'''\n",
    "with open(Path(task2_pred_path) / GROUP_FILE_NAME, 'w', encoding='utf16') as f:\n",
    "    f.write('\\n'.join(answers))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting the Question from The Paragraph\n",
    "Assuming that the right sentence from the right paragraph is given for the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_answer(question, answerSentence):\n",
    "    analysis_result = analyze(answerSentence)\n",
    "    answer_words = []\n",
    "    answer_types = []\n",
    "    a_dic = {}\n",
    "    index = 0\n",
    "    for a in analysis_result.results:\n",
    "        best = a.best\n",
    "        answer_words.append(best.lemmas[0])\n",
    "        answer_types.append(best.pos)\n",
    "        b = []\n",
    "        b.append(index)\n",
    "        b.append(a.token)\n",
    "        b.append(best.lemmas[0])\n",
    "        b.append(best.pos)\n",
    "        index += 1\n",
    "        a_dic[best.lemmas[0]] = b\n",
    "        lemmas = \"\"\n",
    "        for l in best.lemmas:\n",
    "            lemmas = lemmas + \" \" + l\n",
    "        #print(\"Word = \" + a.token + \", Lemmas = \" + lemmas + \", POS = [\" + best.pos + \"], Full Analysis = {\" + best.analysis + \"}\")\n",
    "        \n",
    "    question_words = []\n",
    "    question_types = {}\n",
    "    q = analyze(question)\n",
    "    print(a_dic)\n",
    "    for a in q.results:\n",
    "        best = a.best\n",
    "        question_words.append(best.lemmas[0])\n",
    "        question_types[best.lemmas[0]]=(best.pos)\n",
    "        lemmas = \"\"\n",
    "        for l in best.lemmas:\n",
    "            lemmas = lemmas + \" \" + l\n",
    "        print(\"Word = \" + a.token + \", Lemmas = \" + lemmas + \", POS = [\" + best.pos + \"], Full Analysis = {\" + best.analysis + \"}\")\n",
    "        \n",
    "    l1 = (list(set(question_words)-set(answer_words)))    \n",
    "    l2 = (list(set(answer_words)-set(question_words)))\n",
    "    \n",
    "    l2.sort(key=lambda x: a_dic[x][0], reverse=False)\n",
    "    print(l2)\n",
    "    print(\"\\n--------------\\n\")\n",
    "    \n",
    "    res = []\n",
    "    #print(l1)\n",
    "    if containArr([\"kaç\",\"yıl\",\"kadar\",\"sene\"], question_words):\n",
    "        print(\"a\")\n",
    "        res = ([a_dic[x][1] for x in l2 if a_dic[x][3] == 'Num'])\n",
    "    elif containArr([\"bölge\",\"ülke\",\"kıta\",\"il\", \"ilçe\", \"köy\", \"eyalet\"], question_words):\n",
    "        print(\"b\")\n",
    "\n",
    "        res = ([a_dic[x][1]  for x in l2 if a_dic[x][3] == 'Noun'])\n",
    "    elif \"nasıl\" in question_words:\n",
    "        print(\"c\")\n",
    "\n",
    "        res = ([a_dic[x][1]  for x in l2 if a_dic[x][3] == 'Adj' or a_dic[x][3] == 'Adv' or a_dic[x][3] == 'Verb' ])\n",
    "    elif containArr([\"ad\",\"isim\"], question_words): #bakılacak\n",
    "        print(\"d\")\n",
    "        res = ([a_dic[x][1]  for x in l2 if a_dic[x][3] == 'Noun'])\n",
    "    elif \"kim\" in question_words:\n",
    "        print(\"e\")\n",
    "        res = ([a_dic[x][1]  for x in l2 if a_dic[x][3] == 'Noun'])\n",
    "    elif containArr([\"mı\",\"mi\",\"mu\",\"mü\"],question_words):\n",
    "        if len([a_dic[x][1] for x in l2 if a_dic[x][3] == 'Verb']) != 0:\n",
    "            print(\"f\")\n",
    "            res = ([a_dic[x][1] for x in answer_words if a_dic[x][3] == 'Verb'])\n",
    "        else:\n",
    "            print(\"g\")\n",
    "            res = ([a_dic[x][1]  for x in l2 if a_dic[x][3] =='Adv'])\n",
    "    else:\n",
    "        res = [a_dic[x][1] for x in l2 if (a_dic[x][3] != \"Postp\" and a_dic[x][3] != \"Conj\" and a_dic[x][3] != \"Punc\" and a_dic[x][3] != \"Det\")]\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'istanbul': [0, 'istanbulun', 'istanbul', 'Noun'], 'başkent': [1, 'başkenti', 'başkent', 'Noun'], 'ankara': [2, 'ankarada', 'ankara', 'Noun'], 'yer': [3, 'yer', 'yer', 'Noun'], 'al': [4, 'alır', 'al', 'Verb'], '.': [5, '.', '.', 'Punc']}\n",
      "Word = istanbulun, Lemmas =  istanbul, POS = [Noun], Full Analysis = {[İstanbul:Noun,Prop] istanbul:Noun+A3sg+un:Gen}\n",
      "Word = başkentini, Lemmas =  başkent, POS = [Noun], Full Analysis = {[başkent:Noun] başkent:Noun+A3sg+i:P3sg+ni:Acc}\n",
      "Word = yazınız, Lemmas =  yazı, POS = [Noun], Full Analysis = {[yazı:Noun] yazı:Noun+A3sg+nız:P2pl}\n",
      "Word = ?, Lemmas =  ?, POS = [Punc], Full Analysis = {[?:Punc] ?:Punc}\n",
      "['ankara', 'yer', 'al', '.']\n",
      "\n",
      "--------------\n",
      "\n",
      "['ankarada', 'yer', 'alır']\n"
     ]
    }
   ],
   "source": [
    "find_answer(\"istanbulun başkentini yazınız?\", \"istanbulun başkenti ankarada yer alır.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
