{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "GROUP_FILE_NAME = '2.txt'\n",
    "\n",
    "# _, test_questions_path, task1_pred_path, task2_pred_path = sys.argv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we do not take the argument from command line yet\n",
    "task1_pred_path = cwd + \"/task1_predictions\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatizer from @akoksal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "class Lemmatizer:\n",
    "    def __init__(self):\n",
    "        try:\n",
    "            with open('revisedDict.pkl', 'rb') as f:\n",
    "                self.revisedDict = pickle.load(f)\n",
    "        except IOError:\n",
    "            print(\"Please run trainLexicon.py to generate revisedDict.pkl file\")\n",
    "\n",
    "    def check(self, root, suffix, guess, action):\n",
    "        if action == \"unsuz yumusamasi\":\n",
    "            return len(suffix)>0 and suffix[0] in [\"a\",\"e\",\"ı\",\"i\",\"o\",\"ö\",\"u\",\"ü\"] and self.checkSuffixValidation(suffix)[0]\n",
    "        if action == \"unlu daralmasi\":\n",
    "            if guess==\"demek\" and self.checkSuffixValidation(suffix)[0]:\n",
    "                return True\n",
    "            if guess==\"yemek\" and self.checkSuffixValidation(suffix)[0]:\n",
    "                return True\n",
    "            \n",
    "            if suffix.startswith(\"yor\"):\n",
    "                lastVowel = \"\"\n",
    "                for letter in reversed(guess[:-3]):\n",
    "                    if letter in [\"a\",\"e\",\"ı\",\"i\",\"o\",\"ö\",\"u\",\"ü\"]:\n",
    "                        lastVowel = letter\n",
    "                        break\n",
    "                if lastVowel in [\"a\",\"e\"] and self.checkSuffixValidation(suffix)[0]:\n",
    "                    return True\n",
    "            return False\n",
    "        if action == \"fiil\" or action == \"olumsuzluk eki\":\n",
    "            return self.checkSuffixValidation(suffix)[0] and not ((root.endswith(\"la\") or (root.endswith(\"le\"))) and suffix.startswith(\"r\"))\n",
    "        if action == \"unlu dusmesi\":\n",
    "            count = 0\n",
    "            for letter in guess:\n",
    "                if letter in [\"a\",\"e\",\"ı\",\"i\",\"o\",\"ö\",\"u\",\"ü\"]:\n",
    "                    count+=1\n",
    "                    lastVowel = letter\n",
    "            if self.checkSuffixValidation(suffix)[0] and count==2 and (lastVowel in [\"ı\",\"i\",\"u\",\"ü\"]) and (len(suffix)>0 and suffix[0] in [\"a\",\"e\",\"ı\",\"i\",\"o\",\"ö\",\"u\",\"ü\"]):\n",
    "                if lastVowel == \"ı\":\n",
    "                    return suffix[0] in [\"a\",\"ı\"]\n",
    "                elif lastVowel == \"i\":\n",
    "                    return suffix[0] in [\"e\",\"i\"]\n",
    "                elif lastVowel == \"u\":\n",
    "                    return suffix[0] in [\"a\",\"u\"]\n",
    "                elif lastVowel == \"ü\":\n",
    "                    return suffix[0] in [\"e\",\"ü\"]\n",
    "            return False\n",
    "        return True\n",
    "        \n",
    "    def findPos(self, kelime):\n",
    "        revisedDict = self.revisedDict\n",
    "        l = []\n",
    "        if \"'\" in kelime:\n",
    "            l.append([kelime[:kelime.index(\"'\")]+\"_1\",\"tirnaksiz\",kelime])\n",
    "        mid = []\n",
    "        for i in range(len(kelime)):\n",
    "            guess = kelime[:len(kelime)-i]\n",
    "            suffix = kelime[len(kelime)-i:]\n",
    "            ct = 1\n",
    "            \n",
    "            while guess+\"_\"+str(ct) in revisedDict:\n",
    "                if self.check(guess, suffix, revisedDict[guess+\"_\"+str(ct)][1], revisedDict[guess+\"_\"+str(ct)][0]):\n",
    "                    guessList = (revisedDict[guess+\"_\"+str(ct)])\n",
    "                    while guessList[0] not in [\"kok\",\"fiil\",\"olumsuzluk\"]:\n",
    "                        guessList = revisedDict[guessList[1]]\n",
    "                    mid.append([guessList[1], revisedDict[guess+\"_\"+str(ct)][0],guess+\"_\"+str(ct)])\n",
    "                ct = ct+1\n",
    "                \n",
    "        temp = []\n",
    "        for kel in mid:\n",
    "            kelime_kok = kel[0][:kel[0].index(\"_\")]\n",
    "            kelime_len = len(kelime_kok)\n",
    "            if kelime_kok.endswith(\"mak\") or kelime_kok.endswith(\"mek\"):\n",
    "                kelime_len -= 3\n",
    "            not_inserted = True\n",
    "            for index in range(len(temp)):\n",
    "                temp_kelime = temp[index]\n",
    "                temp_kelime_kok = temp_kelime[0][:temp_kelime[0].index(\"_\")]\n",
    "                temp_len = len(temp_kelime_kok)\n",
    "                if temp_kelime_kok.endswith(\"mak\") or temp_kelime_kok.endswith(\"mek\"):\n",
    "                    temp_len -= 3\n",
    "                if(kelime_len>temp_len):\n",
    "                    temp.insert(index,kel)\n",
    "                    not_inserted = False\n",
    "            if not_inserted:\n",
    "                temp.append(kel)\n",
    "        output = l+temp\n",
    "        if len(output)==0:\n",
    "            output.append([kelime+\"_1\",\"çaresiz\",kelime+\"_1\",])\n",
    "        return output\n",
    "\n",
    "    def checkSuffixValidation(self, suff):\n",
    "        suffixList = [\"\",\"a\", \"abil\", \"acağ\", \"acak\", \"alım\", \"ama\", \"an\", \"ar\", \"arak\", \"asın\", \"asınız\", \"ayım\", \"da\", \"dan\", \"de\", \"den\", \"dı\", \"dığ\", \"dık\", \"dıkça\", \"dır\", \"di\", \"diğ\", \"dik\", \"dikçe\", \"dir\", \"du\", \"duğ\", \"duk\", \"dukça\", \"dur\", \"dü\", \"düğ\", \"dük\", \"dükçe\", \"dür\", \"e\", \"ebil\", \"eceğ\", \"ecek\", \"elim\", \"eme\", \"en\", \"er\", \"erek\", \"esin\", \"esiniz\", \"eyim\", \"ı\", \"ıl\", \"ım\", \"ımız\", \"ın\", \"ınca\", \"ınız\", \"ıp\", \"ır\", \"ıyor\", \"ız\", \"i\", \"il\", \"im\", \"imiz\", \"in\", \"ince\", \"iniz\", \"ip\", \"ir\", \"iyor\", \"iz\", \"k\", \"ken\", \"la\", \"lar\", \"ları\", \"ların\", \"le\", \"ler\", \"leri\", \"lerin\", \"m\", \"ma\", \"madan\", \"mak\", \"maksızın\", \"makta\", \"maktansa\", \"malı\", \"maz\", \"me\", \"meden\", \"mek\", \"meksizin\", \"mekte\", \"mektense\", \"meli\", \"mez\", \"mı\", \"mış\", \"mız\", \"mi\", \"miş\", \"miz\", \"mu\", \"muş\", \"mü\", \"muz\", \"müş\", \"müz\", \"n\", \"nın\", \"nız\", \"nin\", \"niz\", \"nun\", \"nuz\", \"nün\", \"nüz\", \"r\", \"sa\", \"se\", \"sı\", \"sın\", \"sınız\", \"sınlar\", \"si\", \"sin\", \"siniz\", \"sinler\", \"su\", \"sun\", \"sunlar\", \"sunuz\", \"sü\", \"sün\", \"sünler\", \"sünüz\", \"ta\", \"tan\", \"te\", \"ten\", \"tı\", \"tığ\", \"tık\", \"tıkça\", \"tır\", \"ti\", \"tiğ\", \"tik\", \"tikçe\", \"tir\", \"tu\", \"tuğ\", \"tuk\", \"tukça\", \"tur\", \"tü\", \"tüğ\", \"tük\", \"tükçe\", \"tür\", \"u\", \"ul\", \"um\", \"umuz\", \"un\", \"unca\", \"unuz\", \"up\", \"ur\", \"uyor\", \"uz\", \"ü\", \"ül\", \"ün\", \"üm\", \"ümüz\", \"ünce\", \"ünüz\", \"üp\", \"ür\", \"üyor\", \"üz\", \"ya\", \"yabil\", \"yacağ\", \"yacak\", \"yalım\", \"yama\", \"yan\", \"yarak\", \"yasın\", \"yasınız\", \"yayım\", \"ydı\", \"ydi\", \"ydu\", \"ydü\", \"ye\", \"yebil\", \"yeceğ\", \"yecek\", \"yelim\", \"yeme\", \"yen\", \"yerek\", \"yesin\", \"yesiniz\", \"yeyim\", \"yı\", \"yım\", \"yın\", \"yınca\", \"yınız\", \"yıp\", \"yız\", \"yi\", \"yim\", \"yin\", \"yince\", \"yiniz\", \"yip\", \"yiz\", \"yken\", \"yla\", \"yle\", \"ymış\", \"ymiş\", \"ymuş\", \"ymüş\", \"yor\", \"ysa\", \"yse\", \"yu\", \"yum\", \"yun\", \"yunca\", \"yunuz\", \"yup\", \"yü\", \"yuz\", \"yüm\", \"yün\", \"yünce\", \"yünüz\", \"yüp\", \"yüz\"]\n",
    "        validList = []\n",
    "        if suff in suffixList:\n",
    "            validList.append(suff)\n",
    "        for ind in range(1,len(suff)):\n",
    "            if(suff[:ind] in suffixList):\n",
    "                cont, contList = self.checkSuffixValidation(suff[ind:])\n",
    "                if cont:\n",
    "                    contList = [suff[:ind]+\"+\"+l for l in contList]\n",
    "                    validList = validList+contList\n",
    "        return len(validList)>0,validList\n",
    "\n",
    "    def lemmatize(self, text):\n",
    "        dc  = []\n",
    "        for i in text.split(' '):\n",
    "            findings = self.findPos(i.lower())\n",
    "            word = findings[0][0]\n",
    "            dc.append(findings[0][0][:-2])\n",
    "    \n",
    "        return dc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from string import punctuation\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    \"\"\"\n",
    "    Removes punctuations from given text\n",
    "    \"\"\"\n",
    "    # Replace punctuation with space instead of remove it for hand-to-mouth, six-week-old, euro-certificate\n",
    "    return text.translate(str.maketrans(punctuation, ' ' * len(punctuation)))\n",
    "\n",
    "\n",
    "def remove_extra_whitespaces(text):\n",
    "    \"\"\"\n",
    "    Removes extra whitespaces from given text such as multiple adjencent space\n",
    "    \"\"\"\n",
    "    return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "\n",
    "def replace_capital_i(text):\n",
    "    \"\"\"\n",
    "    Replaces İ with i since it gave error on @akoksal 's script.\n",
    "    \"\"\"\n",
    "    return text.replace(\"İ\", \"i\")\n",
    "\n",
    "\n",
    "def normalize(text):\n",
    "    \"\"\"\n",
    "    Normalizes the text.\n",
    "    \"\"\"\n",
    "    return remove_extra_whitespaces(remove_punctuation(replace_capital_i(text))).lower()\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    Returns a word array from the given text.\n",
    "    \"\"\"\n",
    "    foo = text.split(' ')\n",
    "    return foo\n",
    "\n",
    "\n",
    "def tokenize_lemmatize(text):\n",
    "    \"\"\"\n",
    "    Returns a word array from the given text.\n",
    "    \"\"\"\n",
    "    foo = text.split(' ')\n",
    "    new_items = [lemmatizer.lemmatize(i)[0] for i in foo]\n",
    "    return new_items\n",
    "\n",
    "\n",
    "def find_sentence(que, par):\n",
    "    \"\"\"\n",
    "    Returns the related sentence to the question in the paragraph.\n",
    "    \"\"\"\n",
    "    sent = par\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntest_text = \"Demir çelik sanayisinin temel ham maddesi olan ve ülkemizde en çok tüketilen madenlerden biri demirdir. Demir rezervi bakımından en zengin yataklar, Sivas ve Malatya illerinde bulunmaktadır. Divriği, Hasançelebi ve Hekimhan, bu illerde demir yataklarının bulunduğu başlıca yerlerdir. Diğer önemli demir yatakları Niğde, Bingöl ve Kesikköprü’dedir (Ankara).Bu yerlerden çıkarılan demir Ereğli (Zonguldak), Karabük ve İskenderun demir çelik fabrikalarında işlenmektedir.\"\\nlemmatizer = Lemmatizer()\\nprint(lemmatizer.lemmatize(normalize(test_text))) \\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "test_text = \"Demir çelik sanayisinin temel ham maddesi olan ve ülkemizde en çok tüketilen madenlerden biri demirdir. Demir rezervi bakımından en zengin yataklar, Sivas ve Malatya illerinde bulunmaktadır. Divriği, Hasançelebi ve Hekimhan, bu illerde demir yataklarının bulunduğu başlıca yerlerdir. Diğer önemli demir yatakları Niğde, Bingöl ve Kesikköprü’dedir (Ankara).Bu yerlerden çıkarılan demir Ereğli (Zonguldak), Karabük ve İskenderun demir çelik fabrikalarında işlenmektedir.\"\n",
    "lemmatizer = Lemmatizer()\n",
    "print(lemmatizer.lemmatize(normalize(test_text))) \n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Database\n",
    "## Read derlem.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re\n",
    "\n",
    "'''\n",
    "Transfer DERLEM to a Dictionary.\n",
    "'''\n",
    "with open(\"derlem.txt\", encoding=\"utf-16\") as f:\n",
    "    opened_file = f.readlines()\n",
    "\n",
    "'''\n",
    "The dictionary of the DERLEM.\n",
    "{id: paragraph as string}\n",
    "'''\n",
    "file_dict = {}\n",
    "for i, line in enumerate(opened_file):\n",
    "    if line != \"\\n\":\n",
    "        '''\n",
    "        The first number in the line is the Paragraph ID,\n",
    "        assign the Paragraph ID as the key to the dictionary,\n",
    "        and the rest of the paragraph to the value.\n",
    "        '''\n",
    "        file_dict[int(line.split(' ', 1)[0])] = re.findall(\" (.*)\", line)[0]\n",
    "    \n",
    "# print(len(file_dict))\n",
    "\n",
    "'''\n",
    "The normalized dictionary of the DERLEM.\n",
    "{id: array of tokens}\n",
    "'''\n",
    "norm_dict = file_dict.copy()\n",
    "lemmatizer = Lemmatizer()\n",
    "for i in norm_dict:\n",
    "    norm_dict[i] = lemmatizer.lemmatize(normalize(file_dict[i]))\n",
    "    \n",
    "# print(norm_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read soru_gruplari.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the encoding of the input file to latin-1\n",
    "file_content = \"\"\n",
    "with open(\"soru_gruplari.txt\", encoding=\"utf-16\") as f:\n",
    "    file_content = f.read()\n",
    "# Read the file and append all of the lines\n",
    "question_groups = file_content.split(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_groups_dict = {}\n",
    "for group in question_groups:\n",
    "    lines = group.split(\"\\n\")\n",
    "    answer_id = \"\"\n",
    "    group_dict = collections.defaultdict(dict)\n",
    "    for line in lines:\n",
    "        ID = re.findall(\"(.*):\", line)[0]\n",
    "        sentence = re.findall(\": (.*)\", line)[0]\n",
    "        if ID[0] == 'S':  # if the sentence is a question\n",
    "            group_dict[\"questions\"][ID] = sentence\n",
    "        elif ID[0] == 'C':  # if the sentence is the answer\n",
    "            group_dict[\"answer\"] = sentence\n",
    "            answer_id = ID\n",
    "        else:  # if the sentence is the id of related paragraph\n",
    "            group_dict[\"paragraph\"] = sentence\n",
    "    question_groups_dict[answer_id] = group_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraphs_and_terms = {}\n",
    "all_terms = set()\n",
    "for paragraph_id in norm_dict:\n",
    "    terms = norm_dict[paragraph_id]\n",
    "    all_terms.update(terms)\n",
    "    paragraphs_and_terms[paragraph_id] = terms\n",
    "all_terms = list(all_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute term frequency for each term and paragraph\n",
    "tf = {}\n",
    "for paragraph_id in norm_dict:\n",
    "    for term in norm_dict[paragraph_id]:\n",
    "        key = (paragraph_id, term)\n",
    "        tf[key] = norm_dict[paragraph_id].count(term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute document frequency for each term\n",
    "df = {}\n",
    "for term in all_terms:\n",
    "    df[term] = sum([1 for paragraph_id in norm_dict if norm_dict[paragraph_id].count(term) > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tf.idf score for a given query (q) and the set of paragraphs (documents - d) is $\\sum\\limits_{t\\in q\\cap d} tf.idf_{t,d}$ where\n",
    "\n",
    "tf score = $\\begin{cases}\n",
    "  1 + log_{10}tf_{t, d}, & \\text{if $tf_{t, d} > 0$}\\\\\n",
    "  0, & \\text{otherwise}\n",
    "\\end{cases}  $\n",
    "\n",
    "and\n",
    "\n",
    "idf score = $log_{10}(N / df_{t})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import collections\n",
    "\n",
    "# Compute tf-idf scores for each paragraph\n",
    "weights = collections.defaultdict(list)\n",
    "for paragraph_id in norm_dict:\n",
    "    for term in all_terms:\n",
    "        if((paragraph_id, term) in tf):\n",
    "            weights[paragraph_id].append((1 + math.log10(tf[(paragraph_id, term)])) * math.log10(len(norm_dict)/df[term]))\n",
    "        else:\n",
    "            weights[paragraph_id].append(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_related_paragraph(question):\n",
    "    lemmatizer = Lemmatizer()\n",
    "    query_terms = lemmatizer.lemmatize(normalize(question))\n",
    "    \n",
    "    query_term_indices = [all_terms.index(term) for term in query_terms if term in all_terms]\n",
    "\n",
    "    tf_idf = {}\n",
    "    for paragraph_id in weights:\n",
    "        tf_idf[paragraph_id] = sum([weight for index, weight in enumerate(weights[paragraph_id]) if index in query_term_indices])\n",
    "\n",
    "    related_paragraphs = sorted(tf_idf.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return related_paragraphs[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_answer(question):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write predicted results to files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/dilrubareyyan/Codes/dolu-dolu-anadolu/test_questions.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-d0e5f69231d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mrelated_paragraphs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0manswers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcwd\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/test_questions.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf-16\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mquestions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mquestion\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mquestions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/dilrubareyyan/Codes/dolu-dolu-anadolu/test_questions.txt'"
     ]
    }
   ],
   "source": [
    "related_paragraphs = []\n",
    "answers = []\n",
    "with open(cwd + '/test_questions.txt', 'r', encoding=\"utf-16\") as f:\n",
    "    questions = f.readlines()\n",
    "    for question in questions:\n",
    "        paragraph_id = predict_related_paragraph(question)\n",
    "        answer = predict_answer(question)\n",
    "        related_paragraphs.append(str(paragraph_id))\n",
    "        answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the following line if you are running this for the first time\n",
    "# os.mkdir(task1_pred_path)\n",
    "\n",
    "with open(Path(task1_pred_path) / GROUP_FILE_NAME, 'w+', encoding='utf16') as f:\n",
    "    f.write('\\n'.join(related_paragraphs))\n",
    "\n",
    "# TODO: complete task 2    \n",
    "'''\n",
    "with open(Path(task2_pred_path) / GROUP_FILE_NAME, 'w', encoding='utf16') as f:\n",
    "    f.write('\\n'.join(answers))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting the Question from The Paragraph\n",
    "Assuming that the right sentence from the right paragraph is given for the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "almanya da ılıman okyanusal ve karasal iklim şartları hakimdir\n",
      "almanya da hangi iklim şartları hakimdir\n",
      "['almanya', 'da', 'ılıman', 'okyanus', 've', 'karasal', 'iklim', 'şartla', 'hakim']\n",
      "['almanya', 'da', 'hangi', 'iklim', 'şartla', 'hakim']\n",
      "\n",
      " ılıman okyanus ve karasal\n"
     ]
    }
   ],
   "source": [
    "def answer_simple(qu, par):\n",
    "    '''\n",
    "    Assumes that find_sentence returns the related sentence \n",
    "    '''\n",
    "    sent = find_sentence(qu, par)\n",
    "    \n",
    "    qu = normalize(qu)\n",
    "    sent = normalize(sent)\n",
    "\n",
    "    print(sent)\n",
    "    print(qu)\n",
    "\n",
    "    sent = tokenize_lemmatize(sent)\n",
    "    qu = tokenize_lemmatize(qu)\n",
    "\n",
    "    ans = \"\"\n",
    "\n",
    "    print(sent)\n",
    "    print(qu)\n",
    "\n",
    "    for i in sent:\n",
    "        if not (i in qu):\n",
    "            ans += i + \" \"\n",
    "\n",
    "    print(\"\\n\", ans.rstrip())\n",
    "    return ans.rstrip()\n",
    "    \n",
    "    \n",
    "# question = \"Tek ev ve eklentisi şeklindeki yerleşim birimlerine çoğunlukla hangi bölgemizde rastlanır?\"\n",
    "# paragraph = \"Tek ev ve eklentilerinden oluşan yerleşim birimlerine daha çok Karadeniz’de rastlanır (Fotoğraf: 2.31).\"\n",
    "\n",
    "# question = \"12000 yıl öncesine tarihlendirilen hangi yerleşme Şanlıurfa yakınlarında bulunur?\"\n",
    "# paragraph = \"Yapılan araştırmalara göre Şanlıurfa yakınlarındaki Göbeklitepe ülkemizde bilinen en eski yerleşmelerden olup yaklaşık 12.000 yıl öncesine tarihlendirilmektedir.\"\n",
    "\n",
    "question = \"Almanya'da hangi iklim şartları hakimdir?\"\n",
    "paragraph = \"Almanya'da ılıman okyanusal ve karasal iklim şartları hakimdir.\"  \n",
    "\n",
    "answer_simple(question, paragraph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "brew cask install docker\n",
    "\n",
    "open --background -a Docker\n",
    "\n",
    "sudo docker run -d --rm -p 6789:6789 --name zemberek-grpc ryts/zemberek-grpc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install zemberek-grpc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import sys\n",
    "\n",
    "import grpc\n",
    "\n",
    "import zemberek_grpc.language_id_pb2 as z_langid\n",
    "import zemberek_grpc.language_id_pb2_grpc as z_langid_g\n",
    "import zemberek_grpc.normalization_pb2 as z_normalization\n",
    "import zemberek_grpc.normalization_pb2_grpc as z_normalization_g\n",
    "import zemberek_grpc.preprocess_pb2 as z_preprocess\n",
    "import zemberek_grpc.preprocess_pb2_grpc as z_preprocess_g\n",
    "import zemberek_grpc.morphology_pb2 as z_morphology\n",
    "import zemberek_grpc.morphology_pb2_grpc as z_morphology_g\n",
    "\n",
    "channel = grpc.insecure_channel('localhost:6789')\n",
    "\n",
    "langid_stub = z_langid_g.LanguageIdServiceStub(channel)\n",
    "normalization_stub = z_normalization_g.NormalizationServiceStub(channel)\n",
    "preprocess_stub = z_preprocess_g.PreprocessingServiceStub(channel)\n",
    "morphology_stub = z_morphology_g.MorphologyServiceStub(channel)\n",
    "\n",
    "def find_lang_id(i):\n",
    "    response = langid_stub.Detect(z_langid.LanguageIdRequest(input=i))\n",
    "    return response.langId\n",
    "\n",
    "def tokenize(i):\n",
    "    response = preprocess_stub.Tokenize(z_preprocess.TokenizationRequest(input=i))\n",
    "    return response.tokens\n",
    "\n",
    "def normalize(i):\n",
    "    response = normalization_stub.Normalize(z_normalization.NormalizationRequest(input=i))\n",
    "    return response\n",
    "\n",
    "def analyze(i):\n",
    "    response = morphology_stub.AnalyzeSentence(z_morphology.SentenceAnalysisRequest(input=i))\n",
    "    return response;\n",
    "\n",
    "def fix_decode(text):\n",
    "    \"\"\"Pass decode.\"\"\"\n",
    "    if sys.version_info < (3, 0):\n",
    "        return text.decode('utf-8')\n",
    "    else:\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findQuestionType(question):\n",
    "    analysis_result = analyze(question)\n",
    "    \n",
    "    for a in analysis_result:\n",
    "        best = a.best\n",
    "        token = a.token\n",
    "        pos = a.pos\n",
    "        lemma = best.lemmas[0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_answer(question, answerSentence):\n",
    "    analysis_result = analyze(answerSentence)\n",
    "    answer_words = []\n",
    "    answer_types = []\n",
    "    a_dic = {}\n",
    "    index = 0\n",
    "    for a in analysis_result.results:\n",
    "        best = a.best\n",
    "        answer_words.append(best.lemmas[0])\n",
    "        answer_types.append(best.pos)\n",
    "        b = []\n",
    "        b.append(index)\n",
    "        b.append(a.token)\n",
    "        b.append(best.lemmas[0])\n",
    "        b.append(best.pos)\n",
    "        index += 1\n",
    "        a_dic[best.lemmas[0]] = b\n",
    "        lemmas = \"\"\n",
    "        for l in best.lemmas:\n",
    "            lemmas = lemmas + \" \" + l\n",
    "        print(\"Word = \" + a.token + \", Lemmas = \" + lemmas + \", POS = [\" + best.pos + \"], Full Analysis = {\" + best.analysis + \"}\")\n",
    "        \n",
    "    question_words = []\n",
    "    question_types = {}\n",
    "    q = analyze(question)\n",
    "    \n",
    "    for a in q.results:\n",
    "        best = a.best\n",
    "        question_words.append(best.lemmas[0])\n",
    "        question_types[best.lemmas[0]]=(best.pos)\n",
    "        lemmas = \"\"\n",
    "        for l in best.lemmas:\n",
    "            lemmas = lemmas + \" \" + l\n",
    "        print(\"Word = \" + a.token + \", Lemmas = \" + lemmas + \", POS = [\" + best.pos + \"], Full Analysis = {\" + best.analysis + \"}\")\n",
    "        \n",
    "    l1 = (list(set(question_words[:-1])-set(answer_words[:-1])))\n",
    "    \n",
    "    print(\"\\n--------------\\n\", l1)\n",
    "    \n",
    "    l2 = (list(set(answer_words[:-1])-set(question_words[:-1])))\n",
    "    l2.sort(key=lambda x: a_dic[x][0], reverse=False)\n",
    "\n",
    "    print(\"\\n--------------\\n\", [a_dic[x][1] for x in l2 if a_dic[x][3] == \"Unk\" or a_dic[x][3] == \"Noun\" or a_dic[x][3] in [question_types[l1[k]] for k in range(len(l1))]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_answer(\"Kuzey yarım kürede kuzeyden esen rüzgarlar etkili oldukları bölgelerde hava sıcaklığına nasıl etki eder?\", \"Örneğin Kuzey yarım kürede kuzeyden esen rüzgârlar etkili oldukları bölgelerde hava sıcaklığını düşürür.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
