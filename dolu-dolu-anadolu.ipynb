{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative verbs are added.\n",
      "Zero infinitive forms of verbs are added.\n",
      "Consonant softening forms are added.\n",
      "Dropping vowel forms are added.\n",
      "Becoming close vowel forms are added.\n",
      "Transformed lexicon is saved to revisedDict.pkl\n"
     ]
    }
   ],
   "source": [
    "%run -i '/Users/dilrubareyyan/Desktop/Turkish-Lemmatizer/trainLexicon.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatizer from @akoksal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "class Lemmatizer:\n",
    "    def __init__(self):\n",
    "        try:\n",
    "            with open('revisedDict.pkl', 'rb') as f:\n",
    "                self.revisedDict = pickle.load(f)\n",
    "        except IOError:\n",
    "            print(\"Please run trainLexicon.py to generate revisedDict.pkl file\")\n",
    "\n",
    "    def check(self, root, suffix, guess, action):\n",
    "        if action == \"unsuz yumusamasi\":\n",
    "            return len(suffix)>0 and suffix[0] in [\"a\",\"e\",\"ı\",\"i\",\"o\",\"ö\",\"u\",\"ü\"] and self.checkSuffixValidation(suffix)[0]\n",
    "        if action == \"unlu daralmasi\":\n",
    "            if guess==\"demek\" and self.checkSuffixValidation(suffix)[0]:\n",
    "                return True\n",
    "            if guess==\"yemek\" and self.checkSuffixValidation(suffix)[0]:\n",
    "                return True\n",
    "            \n",
    "            if suffix.startswith(\"yor\"):\n",
    "                lastVowel = \"\"\n",
    "                for letter in reversed(guess[:-3]):\n",
    "                    if letter in [\"a\",\"e\",\"ı\",\"i\",\"o\",\"ö\",\"u\",\"ü\"]:\n",
    "                        lastVowel = letter\n",
    "                        break\n",
    "                if lastVowel in [\"a\",\"e\"] and self.checkSuffixValidation(suffix)[0]:\n",
    "                    return True\n",
    "            return False\n",
    "        if action == \"fiil\" or action == \"olumsuzluk eki\":\n",
    "            return self.checkSuffixValidation(suffix)[0] and not ((root.endswith(\"la\") or (root.endswith(\"le\"))) and suffix.startswith(\"r\"))\n",
    "        if action == \"unlu dusmesi\":\n",
    "            count = 0\n",
    "            for letter in guess:\n",
    "                if letter in [\"a\",\"e\",\"ı\",\"i\",\"o\",\"ö\",\"u\",\"ü\"]:\n",
    "                    count+=1\n",
    "                    lastVowel = letter\n",
    "            if self.checkSuffixValidation(suffix)[0] and count==2 and (lastVowel in [\"ı\",\"i\",\"u\",\"ü\"]) and (len(suffix)>0 and suffix[0] in [\"a\",\"e\",\"ı\",\"i\",\"o\",\"ö\",\"u\",\"ü\"]):\n",
    "                if lastVowel == \"ı\":\n",
    "                    return suffix[0] in [\"a\",\"ı\"]\n",
    "                elif lastVowel == \"i\":\n",
    "                    return suffix[0] in [\"e\",\"i\"]\n",
    "                elif lastVowel == \"u\":\n",
    "                    return suffix[0] in [\"a\",\"u\"]\n",
    "                elif lastVowel == \"ü\":\n",
    "                    return suffix[0] in [\"e\",\"ü\"]\n",
    "            return False\n",
    "        return True\n",
    "        \n",
    "    def findPos(self, kelime):\n",
    "        revisedDict = self.revisedDict\n",
    "        l = []\n",
    "        if \"'\" in kelime:\n",
    "            l.append([kelime[:kelime.index(\"'\")]+\"_1\",\"tirnaksiz\",kelime])\n",
    "        mid = []\n",
    "        for i in range(len(kelime)):\n",
    "            guess = kelime[:len(kelime)-i]\n",
    "            suffix = kelime[len(kelime)-i:]\n",
    "            ct = 1\n",
    "            \n",
    "            while guess+\"_\"+str(ct) in revisedDict:\n",
    "                if self.check(guess, suffix, revisedDict[guess+\"_\"+str(ct)][1], revisedDict[guess+\"_\"+str(ct)][0]):\n",
    "                    guessList = (revisedDict[guess+\"_\"+str(ct)])\n",
    "                    while guessList[0] not in [\"kok\",\"fiil\",\"olumsuzluk\"]:\n",
    "                        guessList = revisedDict[guessList[1]]\n",
    "                    mid.append([guessList[1], revisedDict[guess+\"_\"+str(ct)][0],guess+\"_\"+str(ct)])\n",
    "                ct = ct+1\n",
    "                \n",
    "        temp = []\n",
    "        for kel in mid:\n",
    "            kelime_kok = kel[0][:kel[0].index(\"_\")]\n",
    "            kelime_len = len(kelime_kok)\n",
    "            if kelime_kok.endswith(\"mak\") or kelime_kok.endswith(\"mek\"):\n",
    "                kelime_len -= 3\n",
    "            not_inserted = True\n",
    "            for index in range(len(temp)):\n",
    "                temp_kelime = temp[index]\n",
    "                temp_kelime_kok = temp_kelime[0][:temp_kelime[0].index(\"_\")]\n",
    "                temp_len = len(temp_kelime_kok)\n",
    "                if temp_kelime_kok.endswith(\"mak\") or temp_kelime_kok.endswith(\"mek\"):\n",
    "                    temp_len -= 3\n",
    "                if(kelime_len>temp_len):\n",
    "                    temp.insert(index,kel)\n",
    "                    not_inserted = False\n",
    "            if not_inserted:\n",
    "                temp.append(kel)\n",
    "        output = l+temp\n",
    "        if len(output)==0:\n",
    "            output.append([kelime+\"_1\",\"çaresiz\",kelime+\"_1\",])\n",
    "        return output\n",
    "\n",
    "    def checkSuffixValidation(self, suff):\n",
    "        suffixList = [\"\",\"a\", \"abil\", \"acağ\", \"acak\", \"alım\", \"ama\", \"an\", \"ar\", \"arak\", \"asın\", \"asınız\", \"ayım\", \"da\", \"dan\", \"de\", \"den\", \"dı\", \"dığ\", \"dık\", \"dıkça\", \"dır\", \"di\", \"diğ\", \"dik\", \"dikçe\", \"dir\", \"du\", \"duğ\", \"duk\", \"dukça\", \"dur\", \"dü\", \"düğ\", \"dük\", \"dükçe\", \"dür\", \"e\", \"ebil\", \"eceğ\", \"ecek\", \"elim\", \"eme\", \"en\", \"er\", \"erek\", \"esin\", \"esiniz\", \"eyim\", \"ı\", \"ıl\", \"ım\", \"ımız\", \"ın\", \"ınca\", \"ınız\", \"ıp\", \"ır\", \"ıyor\", \"ız\", \"i\", \"il\", \"im\", \"imiz\", \"in\", \"ince\", \"iniz\", \"ip\", \"ir\", \"iyor\", \"iz\", \"k\", \"ken\", \"la\", \"lar\", \"ları\", \"ların\", \"le\", \"ler\", \"leri\", \"lerin\", \"m\", \"ma\", \"madan\", \"mak\", \"maksızın\", \"makta\", \"maktansa\", \"malı\", \"maz\", \"me\", \"meden\", \"mek\", \"meksizin\", \"mekte\", \"mektense\", \"meli\", \"mez\", \"mı\", \"mış\", \"mız\", \"mi\", \"miş\", \"miz\", \"mu\", \"muş\", \"mü\", \"muz\", \"müş\", \"müz\", \"n\", \"nın\", \"nız\", \"nin\", \"niz\", \"nun\", \"nuz\", \"nün\", \"nüz\", \"r\", \"sa\", \"se\", \"sı\", \"sın\", \"sınız\", \"sınlar\", \"si\", \"sin\", \"siniz\", \"sinler\", \"su\", \"sun\", \"sunlar\", \"sunuz\", \"sü\", \"sün\", \"sünler\", \"sünüz\", \"ta\", \"tan\", \"te\", \"ten\", \"tı\", \"tığ\", \"tık\", \"tıkça\", \"tır\", \"ti\", \"tiğ\", \"tik\", \"tikçe\", \"tir\", \"tu\", \"tuğ\", \"tuk\", \"tukça\", \"tur\", \"tü\", \"tüğ\", \"tük\", \"tükçe\", \"tür\", \"u\", \"ul\", \"um\", \"umuz\", \"un\", \"unca\", \"unuz\", \"up\", \"ur\", \"uyor\", \"uz\", \"ü\", \"ül\", \"ün\", \"üm\", \"ümüz\", \"ünce\", \"ünüz\", \"üp\", \"ür\", \"üyor\", \"üz\", \"ya\", \"yabil\", \"yacağ\", \"yacak\", \"yalım\", \"yama\", \"yan\", \"yarak\", \"yasın\", \"yasınız\", \"yayım\", \"ydı\", \"ydi\", \"ydu\", \"ydü\", \"ye\", \"yebil\", \"yeceğ\", \"yecek\", \"yelim\", \"yeme\", \"yen\", \"yerek\", \"yesin\", \"yesiniz\", \"yeyim\", \"yı\", \"yım\", \"yın\", \"yınca\", \"yınız\", \"yıp\", \"yız\", \"yi\", \"yim\", \"yin\", \"yince\", \"yiniz\", \"yip\", \"yiz\", \"yken\", \"yla\", \"yle\", \"ymış\", \"ymiş\", \"ymuş\", \"ymüş\", \"yor\", \"ysa\", \"yse\", \"yu\", \"yum\", \"yun\", \"yunca\", \"yunuz\", \"yup\", \"yü\", \"yuz\", \"yüm\", \"yün\", \"yünce\", \"yünüz\", \"yüp\", \"yüz\"]\n",
    "        validList = []\n",
    "        if suff in suffixList:\n",
    "            validList.append(suff)\n",
    "        for ind in range(1,len(suff)):\n",
    "            if(suff[:ind] in suffixList):\n",
    "                cont, contList = self.checkSuffixValidation(suff[ind:])\n",
    "                if cont:\n",
    "                    contList = [suff[:ind]+\"+\"+l for l in contList]\n",
    "                    validList = validList+contList\n",
    "        return len(validList)>0,validList\n",
    "\n",
    "    def lemmatize(self, text):\n",
    "        dc  = []\n",
    "        for i in text.split(' '):\n",
    "            findings = self.findPos(i.lower())\n",
    "            word = findings[0][0]\n",
    "            dc.append(findings[0][0][:-2])\n",
    "    \n",
    "        return dc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from string import punctuation\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    \"\"\"\n",
    "    Removes punctuations from given text\n",
    "    \"\"\"\n",
    "    # Replace punctuation with space instead of remove it for hand-to-mouth, six-week-old, euro-certificate\n",
    "    return text.translate(str.maketrans(punctuation, ' ' * len(punctuation)))\n",
    "\n",
    "\n",
    "def remove_extra_whitespaces(text):\n",
    "    \"\"\"\n",
    "    Removes extra whitespaces from given text such as multiple adjencent space\n",
    "    \"\"\"\n",
    "    return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "\n",
    "def normalize(text):\n",
    "    \"\"\"\n",
    "    Normalizes the text.\n",
    "    \"\"\"\n",
    "    return remove_extra_whitespaces(remove_punctuation(text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['demir', 'çelik', 'sanayi', 'temel', 'ham', 'madde', 'ol', 've', 'ülkem', 'en', 'çok', 'tüket', 'maden', 'biri', 'demir', 'demir', 'rezerv', 'bakım', 'en', 'zengin', 'yatak', 'sivas', 've', 'malatya', 'il', 'bulun', 'divriği', 'hasan', 've', 'hekimhan', 'bu', 'il', 'demir', 'yatak', 'bulun', 'başlıca', 'yer', 'diğer', 'önemli', 'demir', 'yatak', 'niğde', 'bingöl', 've', 'kesik', 'ankara', 'bu', 'yer', 'çıkar', 'demir', 'ereğli', 'zonguldak', 'karabük', 've', 'i', 'demir', 'çelik', 'fabrika', 'işle']\n"
     ]
    }
   ],
   "source": [
    "test_text = \"Demir çelik sanayisinin temel ham maddesi olan ve ülkemizde en çok tüketilen madenlerden biri demirdir. Demir rezervi bakımından en zengin yataklar, Sivas ve Malatya illerinde bulunmaktadır. Divriği, Hasançelebi ve Hekimhan, bu illerde demir yataklarının bulunduğu başlıca yerlerdir. Diğer önemli demir yatakları Niğde, Bingöl ve Kesikköprü’dedir (Ankara).Bu yerlerden çıkarılan demir Ereğli (Zonguldak), Karabük ve İskenderun demir çelik fabrikalarında işlenmektedir.\"\n",
    "lemmatizer = Lemmatizer()\n",
    "print(lemmatizer.lemmatize(normalize(test_text))) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re\n",
    "\n",
    "'''\n",
    "Transfer DERLEM to a Dictionary.\n",
    "'''\n",
    "with open(\"derlem.txt\", encoding=\"utf-16\") as f:\n",
    "    opened_file = f.readlines()\n",
    "   \n",
    "'''\n",
    "The dictionary of the DERLEM.\n",
    "{id: paragraph as string}\n",
    "'''\n",
    "file_dict = {}\n",
    "for i, line in enumerate(opened_file):\n",
    "    if line != \"\\n\":\n",
    "        '''\n",
    "        The first number in the line is the Paragraph ID,\n",
    "        assign the Paragraph ID as the key to the dictionary,\n",
    "        and the rest of the paragraph to the value.\n",
    "        '''\n",
    "        file_dict[int(line.split(' ', 1)[0])] = re.findall(\" (.*)\", line)[0]\n",
    "    \n",
    "# print(len(file_dict))\n",
    "\n",
    "'''\n",
    "The normalized dictionary of the DERLEM.\n",
    "{id: array of tokens}\n",
    "'''\n",
    "norm_dict = file_dict.copy()\n",
    "for i in norm_dict:\n",
    "    norm_dict[i] = lemmatizer.lemmatize(normalize(file_dict[i]))\n",
    "    \n",
    "# print(norm_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
