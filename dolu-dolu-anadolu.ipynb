{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "cwd = os.getcwd()\n",
    "file_name = cwd + '/trainLexicon.py'\n",
    "\n",
    "os.system('python3 ' + file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatizer from @akoksal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "class Lemmatizer:\n",
    "    def __init__(self):\n",
    "        try:\n",
    "            with open('revisedDict.pkl', 'rb') as f:\n",
    "                self.revisedDict = pickle.load(f)\n",
    "        except IOError:\n",
    "            print(\"Please run trainLexicon.py to generate revisedDict.pkl file\")\n",
    "\n",
    "    def check(self, root, suffix, guess, action):\n",
    "        if action == \"unsuz yumusamasi\":\n",
    "            return len(suffix)>0 and suffix[0] in [\"a\",\"e\",\"ı\",\"i\",\"o\",\"ö\",\"u\",\"ü\"] and self.checkSuffixValidation(suffix)[0]\n",
    "        if action == \"unlu daralmasi\":\n",
    "            if guess==\"demek\" and self.checkSuffixValidation(suffix)[0]:\n",
    "                return True\n",
    "            if guess==\"yemek\" and self.checkSuffixValidation(suffix)[0]:\n",
    "                return True\n",
    "            \n",
    "            if suffix.startswith(\"yor\"):\n",
    "                lastVowel = \"\"\n",
    "                for letter in reversed(guess[:-3]):\n",
    "                    if letter in [\"a\",\"e\",\"ı\",\"i\",\"o\",\"ö\",\"u\",\"ü\"]:\n",
    "                        lastVowel = letter\n",
    "                        break\n",
    "                if lastVowel in [\"a\",\"e\"] and self.checkSuffixValidation(suffix)[0]:\n",
    "                    return True\n",
    "            return False\n",
    "        if action == \"fiil\" or action == \"olumsuzluk eki\":\n",
    "            return self.checkSuffixValidation(suffix)[0] and not ((root.endswith(\"la\") or (root.endswith(\"le\"))) and suffix.startswith(\"r\"))\n",
    "        if action == \"unlu dusmesi\":\n",
    "            count = 0\n",
    "            for letter in guess:\n",
    "                if letter in [\"a\",\"e\",\"ı\",\"i\",\"o\",\"ö\",\"u\",\"ü\"]:\n",
    "                    count+=1\n",
    "                    lastVowel = letter\n",
    "            if self.checkSuffixValidation(suffix)[0] and count==2 and (lastVowel in [\"ı\",\"i\",\"u\",\"ü\"]) and (len(suffix)>0 and suffix[0] in [\"a\",\"e\",\"ı\",\"i\",\"o\",\"ö\",\"u\",\"ü\"]):\n",
    "                if lastVowel == \"ı\":\n",
    "                    return suffix[0] in [\"a\",\"ı\"]\n",
    "                elif lastVowel == \"i\":\n",
    "                    return suffix[0] in [\"e\",\"i\"]\n",
    "                elif lastVowel == \"u\":\n",
    "                    return suffix[0] in [\"a\",\"u\"]\n",
    "                elif lastVowel == \"ü\":\n",
    "                    return suffix[0] in [\"e\",\"ü\"]\n",
    "            return False\n",
    "        return True\n",
    "        \n",
    "    def findPos(self, kelime):\n",
    "        revisedDict = self.revisedDict\n",
    "        l = []\n",
    "        if \"'\" in kelime:\n",
    "            l.append([kelime[:kelime.index(\"'\")]+\"_1\",\"tirnaksiz\",kelime])\n",
    "        mid = []\n",
    "        for i in range(len(kelime)):\n",
    "            guess = kelime[:len(kelime)-i]\n",
    "            suffix = kelime[len(kelime)-i:]\n",
    "            ct = 1\n",
    "            \n",
    "            while guess+\"_\"+str(ct) in revisedDict:\n",
    "                if self.check(guess, suffix, revisedDict[guess+\"_\"+str(ct)][1], revisedDict[guess+\"_\"+str(ct)][0]):\n",
    "                    guessList = (revisedDict[guess+\"_\"+str(ct)])\n",
    "                    while guessList[0] not in [\"kok\",\"fiil\",\"olumsuzluk\"]:\n",
    "                        guessList = revisedDict[guessList[1]]\n",
    "                    mid.append([guessList[1], revisedDict[guess+\"_\"+str(ct)][0],guess+\"_\"+str(ct)])\n",
    "                ct = ct+1\n",
    "                \n",
    "        temp = []\n",
    "        for kel in mid:\n",
    "            kelime_kok = kel[0][:kel[0].index(\"_\")]\n",
    "            kelime_len = len(kelime_kok)\n",
    "            if kelime_kok.endswith(\"mak\") or kelime_kok.endswith(\"mek\"):\n",
    "                kelime_len -= 3\n",
    "            not_inserted = True\n",
    "            for index in range(len(temp)):\n",
    "                temp_kelime = temp[index]\n",
    "                temp_kelime_kok = temp_kelime[0][:temp_kelime[0].index(\"_\")]\n",
    "                temp_len = len(temp_kelime_kok)\n",
    "                if temp_kelime_kok.endswith(\"mak\") or temp_kelime_kok.endswith(\"mek\"):\n",
    "                    temp_len -= 3\n",
    "                if(kelime_len>temp_len):\n",
    "                    temp.insert(index,kel)\n",
    "                    not_inserted = False\n",
    "            if not_inserted:\n",
    "                temp.append(kel)\n",
    "        output = l+temp\n",
    "        if len(output)==0:\n",
    "            output.append([kelime+\"_1\",\"çaresiz\",kelime+\"_1\",])\n",
    "        return output\n",
    "\n",
    "    def checkSuffixValidation(self, suff):\n",
    "        suffixList = [\"\",\"a\", \"abil\", \"acağ\", \"acak\", \"alım\", \"ama\", \"an\", \"ar\", \"arak\", \"asın\", \"asınız\", \"ayım\", \"da\", \"dan\", \"de\", \"den\", \"dı\", \"dığ\", \"dık\", \"dıkça\", \"dır\", \"di\", \"diğ\", \"dik\", \"dikçe\", \"dir\", \"du\", \"duğ\", \"duk\", \"dukça\", \"dur\", \"dü\", \"düğ\", \"dük\", \"dükçe\", \"dür\", \"e\", \"ebil\", \"eceğ\", \"ecek\", \"elim\", \"eme\", \"en\", \"er\", \"erek\", \"esin\", \"esiniz\", \"eyim\", \"ı\", \"ıl\", \"ım\", \"ımız\", \"ın\", \"ınca\", \"ınız\", \"ıp\", \"ır\", \"ıyor\", \"ız\", \"i\", \"il\", \"im\", \"imiz\", \"in\", \"ince\", \"iniz\", \"ip\", \"ir\", \"iyor\", \"iz\", \"k\", \"ken\", \"la\", \"lar\", \"ları\", \"ların\", \"le\", \"ler\", \"leri\", \"lerin\", \"m\", \"ma\", \"madan\", \"mak\", \"maksızın\", \"makta\", \"maktansa\", \"malı\", \"maz\", \"me\", \"meden\", \"mek\", \"meksizin\", \"mekte\", \"mektense\", \"meli\", \"mez\", \"mı\", \"mış\", \"mız\", \"mi\", \"miş\", \"miz\", \"mu\", \"muş\", \"mü\", \"muz\", \"müş\", \"müz\", \"n\", \"nın\", \"nız\", \"nin\", \"niz\", \"nun\", \"nuz\", \"nün\", \"nüz\", \"r\", \"sa\", \"se\", \"sı\", \"sın\", \"sınız\", \"sınlar\", \"si\", \"sin\", \"siniz\", \"sinler\", \"su\", \"sun\", \"sunlar\", \"sunuz\", \"sü\", \"sün\", \"sünler\", \"sünüz\", \"ta\", \"tan\", \"te\", \"ten\", \"tı\", \"tığ\", \"tık\", \"tıkça\", \"tır\", \"ti\", \"tiğ\", \"tik\", \"tikçe\", \"tir\", \"tu\", \"tuğ\", \"tuk\", \"tukça\", \"tur\", \"tü\", \"tüğ\", \"tük\", \"tükçe\", \"tür\", \"u\", \"ul\", \"um\", \"umuz\", \"un\", \"unca\", \"unuz\", \"up\", \"ur\", \"uyor\", \"uz\", \"ü\", \"ül\", \"ün\", \"üm\", \"ümüz\", \"ünce\", \"ünüz\", \"üp\", \"ür\", \"üyor\", \"üz\", \"ya\", \"yabil\", \"yacağ\", \"yacak\", \"yalım\", \"yama\", \"yan\", \"yarak\", \"yasın\", \"yasınız\", \"yayım\", \"ydı\", \"ydi\", \"ydu\", \"ydü\", \"ye\", \"yebil\", \"yeceğ\", \"yecek\", \"yelim\", \"yeme\", \"yen\", \"yerek\", \"yesin\", \"yesiniz\", \"yeyim\", \"yı\", \"yım\", \"yın\", \"yınca\", \"yınız\", \"yıp\", \"yız\", \"yi\", \"yim\", \"yin\", \"yince\", \"yiniz\", \"yip\", \"yiz\", \"yken\", \"yla\", \"yle\", \"ymış\", \"ymiş\", \"ymuş\", \"ymüş\", \"yor\", \"ysa\", \"yse\", \"yu\", \"yum\", \"yun\", \"yunca\", \"yunuz\", \"yup\", \"yü\", \"yuz\", \"yüm\", \"yün\", \"yünce\", \"yünüz\", \"yüp\", \"yüz\"]\n",
    "        validList = []\n",
    "        if suff in suffixList:\n",
    "            validList.append(suff)\n",
    "        for ind in range(1,len(suff)):\n",
    "            if(suff[:ind] in suffixList):\n",
    "                cont, contList = self.checkSuffixValidation(suff[ind:])\n",
    "                if cont:\n",
    "                    contList = [suff[:ind]+\"+\"+l for l in contList]\n",
    "                    validList = validList+contList\n",
    "        return len(validList)>0,validList\n",
    "\n",
    "    def lemmatize(self, text):\n",
    "        dc  = []\n",
    "        for i in text.split(' '):\n",
    "            findings = self.findPos(i.lower())\n",
    "            word = findings[0][0]\n",
    "            dc.append(findings[0][0][:-2])\n",
    "    \n",
    "        return dc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from string import punctuation\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    \"\"\"\n",
    "    Removes punctuations from given text\n",
    "    \"\"\"\n",
    "    # Replace punctuation with space instead of remove it for hand-to-mouth, six-week-old, euro-certificate\n",
    "    return text.translate(str.maketrans(punctuation, ' ' * len(punctuation)))\n",
    "\n",
    "\n",
    "def remove_extra_whitespaces(text):\n",
    "    \"\"\"\n",
    "    Removes extra whitespaces from given text such as multiple adjencent space\n",
    "    \"\"\"\n",
    "    return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "def replace_capital_i(text):\n",
    "    \"\"\"\n",
    "    Replaces İ with i\n",
    "    \"\"\"\n",
    "    return text.replace(\"İ\", \"i\")\n",
    "\n",
    "def normalize(text):\n",
    "    \"\"\"\n",
    "    Normalizes the text.\n",
    "    \"\"\"\n",
    "    return remove_extra_whitespaces(remove_punctuation(replace_capital_i(text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['demir', 'çelik', 'sanayi', 'temel', 'ham', 'madde', 'ol', 've', 'ülkem', 'en', 'çok', 'tüket', 'maden', 'biri', 'demir', 'demir', 'rezerv', 'bakım', 'en', 'zengin', 'yatak', 'sivas', 've', 'malatya', 'il', 'bulun', 'divriği', 'hasan', 've', 'hekimhan', 'bu', 'il', 'demir', 'yatak', 'bulun', 'başlıca', 'yer', 'diğer', 'önemli', 'demir', 'yatak', 'niğde', 'bingöl', 've', 'kesik', 'ankara', 'bu', 'yer', 'çıkar', 'demir', 'ereğli', 'zonguldak', 'karabük', 've', 'iskenderun', 'demir', 'çelik', 'fabrika', 'işle']\n"
     ]
    }
   ],
   "source": [
    "test_text = \"Demir çelik sanayisinin temel ham maddesi olan ve ülkemizde en çok tüketilen madenlerden biri demirdir. Demir rezervi bakımından en zengin yataklar, Sivas ve Malatya illerinde bulunmaktadır. Divriği, Hasançelebi ve Hekimhan, bu illerde demir yataklarının bulunduğu başlıca yerlerdir. Diğer önemli demir yatakları Niğde, Bingöl ve Kesikköprü’dedir (Ankara).Bu yerlerden çıkarılan demir Ereğli (Zonguldak), Karabük ve İskenderun demir çelik fabrikalarında işlenmektedir.\"\n",
    "lemmatizer = Lemmatizer()\n",
    "print(lemmatizer.lemmatize(normalize(test_text))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Database\n",
    "## Read derlem.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re\n",
    "\n",
    "'''\n",
    "Transfer DERLEM to a Dictionary.\n",
    "'''\n",
    "with open(\"derlem.txt\", encoding=\"utf-16\") as f:\n",
    "    opened_file = f.readlines()\n",
    "   \n",
    "'''\n",
    "The dictionary of the DERLEM.\n",
    "{id: paragraph as string}\n",
    "'''\n",
    "file_dict = {}\n",
    "for i, line in enumerate(opened_file):\n",
    "    if line != \"\\n\":\n",
    "        '''\n",
    "        The first number in the line is the Paragraph ID,\n",
    "        assign the Paragraph ID as the key to the dictionary,\n",
    "        and the rest of the paragraph to the value.\n",
    "        '''\n",
    "        file_dict[int(line.split(' ', 1)[0])] = re.findall(\" (.*)\", line)[0]\n",
    "    \n",
    "# print(len(file_dict))\n",
    "\n",
    "'''\n",
    "The normalized dictionary of the DERLEM.\n",
    "{id: array of tokens}\n",
    "'''\n",
    "norm_dict = file_dict.copy()\n",
    "for i in norm_dict:\n",
    "    norm_dict[i] = lemmatizer.lemmatize(normalize(file_dict[i]))\n",
    "    \n",
    "# print(norm_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read soru_gruplari.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the encoding of the input file to latin-1\n",
    "file_content = \"\"\n",
    "with open(\"soru_gruplari.txt\", encoding=\"utf-16\") as f:\n",
    "    file_content = f.read()\n",
    "# Read the file and append all of the lines\n",
    "question_groups = file_content.split(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_groups_dict = {}\n",
    "for group in question_groups:\n",
    "    lines = group.split(\"\\n\")\n",
    "    answer_id = \"\"\n",
    "    group_dict = collections.defaultdict(dict)\n",
    "    for line in lines:\n",
    "        ID = re.findall(\"(.*):\", line)[0]\n",
    "        sentence = re.findall(\": (.*)\", line)[0]\n",
    "        if ID[0] == 'S':  # if the sentence is a question\n",
    "            group_dict[\"questions\"][ID] = sentence\n",
    "        elif ID[0] == 'C':  # if the sentence is the answer\n",
    "            group_dict[\"answer\"] = sentence\n",
    "            answer_id = ID\n",
    "        else:  # if the sentence is the id of related paragraph\n",
    "            group_dict[\"paragraph\"] = sentence\n",
    "    question_groups_dict[answer_id] = group_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraphs_and_terms = {}\n",
    "all_terms = set()\n",
    "for paragraph_id in norm_dict:\n",
    "    terms = norm_dict[paragraph_id]\n",
    "    all_terms.update(terms)\n",
    "    paragraphs_and_terms[paragraph_id] = terms\n",
    "all_terms = list(all_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute term frequency for each term and paragraph\n",
    "tf = {}\n",
    "for paragraph_id in norm_dict:\n",
    "    for term in norm_dict[paragraph_id]:\n",
    "        key = (paragraph_id, term)\n",
    "        tf[key] = norm_dict[paragraph_id].count(term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute document frequency for each term\n",
    "df = {}\n",
    "for term in all_terms:\n",
    "    df[term] = sum([1 for paragraph_id in norm_dict if norm_dict[paragraph_id].count(term) > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tf.idf score for a given query (q) and the set of paragraphs (documents - d) is $\\sum\\limits_{t\\in q\\cap d} tf.idf_{t,d}$ where\n",
    "\n",
    "tf score = $\\begin{cases}\n",
    "  1 + log_{10}tf_{t, d}, & \\text{if $tf_{t, d} > 0$}\\\\\n",
    "  0, & \\text{otherwise}\n",
    "\\end{cases}  $\n",
    "\n",
    "and\n",
    "\n",
    "idf score = $log_{10}(N / df_{t})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import collections\n",
    "\n",
    "# Compute tf-idf scores for each paragraph\n",
    "weights = collections.defaultdict(list)\n",
    "for paragraph_id in norm_dict:\n",
    "    for term in all_terms:\n",
    "        if((paragraph_id, term) in tf):\n",
    "            weights[paragraph_id].append((1 + math.log10(tf[(paragraph_id, term)])) * math.log10(len(norm_dict)/df[term]))\n",
    "        else:\n",
    "            weights[paragraph_id].append(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_query = \"Dünyada altın arama ve üretme işlemi büyük ölçüde neyle yapılmaktadır?\"\n",
    "\n",
    "lemmatizer = Lemmatizer()\n",
    "query_terms = lemmatizer.lemmatize(normalize(example_query)) \n",
    "\n",
    "query_term_indices = [all_terms.index(term) for term in query_terms]\n",
    "\n",
    "tf_idf = {}\n",
    "for paragraph_id in weights:\n",
    "    tf_idf[paragraph_id] = sum([weight for index, weight in enumerate(weights[paragraph_id]) if index in query_term_indices])\n",
    "    \n",
    "related_paragraphs = sorted(tf_idf.items(), key=lambda x: x[1], reverse=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
